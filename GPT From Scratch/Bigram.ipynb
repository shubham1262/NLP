{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.4.0-cp311-cp311-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.4/761.4 kB\u001b[0m \u001b[31m851.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2023.6.3-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m78.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /Users/prashant.singh/anaconda3/envs/pytorch/lib/python3.11/site-packages (from tiktoken) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/prashant.singh/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/prashant.singh/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/prashant.singh/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/prashant.singh/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n",
      "Installing collected packages: regex, tiktoken\n",
      "Successfully installed regex-2023.6.3 tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "#Download as input.txt: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# !pip install tiktoken #https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "from itertools import chain #Deal with nested list\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading file \n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_manual=len(list(set(text))) #Character level tokenization\n",
    "vocab_size_manual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization (Tiktoken, char level)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6151, 1070]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# Tokenize: Convert raw string into set of integers according to some vocabulary\n",
    "# Small vocab means large set of integers and vice versa\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "print(enc.encode('hi there'))\n",
    "print(enc.decode([6151,1070]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size_tiktoken = len(set(enc.encode(text)))\n",
    "vocab_size_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100252"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(set(enc.encode(text)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually build vocab and encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# All the unique characters that appear in the text\n",
    "chars=sorted(list(set(text)))\n",
    "print(''.join(chars))\n",
    "vocab_size=len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi={}\n",
    "for i,ch in enumerate(chars):\n",
    "    stoi[ch]=i\n",
    "\n",
    "itos={}\n",
    "for i,ch in enumerate(chars):\n",
    "    itos[i]=ch\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] #Creating a function to output a list of integers for encoding\n",
    "decode = lambda s: ''.join([itos[c] for c in s]) #Creating a function to output a list of integers for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hi there'))\n",
    "print(decode([46, 47, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# data=torch.tensor(encode(text),dtype=torch.long)\n",
    "# print(data.shape,data.dtype)\n",
    "\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape,data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(data[:10].tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "test_data=data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Training\n",
    "* While trainig we dont pass all the data at once, we train using block (or chunks of data). Context_length/block size\n",
    "* Train it to predict at every position word\n",
    "* In the context of 5451, 47317 comes next. In the context of 5451 and 47317, 512 comes next. So we have 8 positions.\n",
    "* When we train we pass in multiple chunks of text stacked up, just so we keep GPUs busy (leverage parallel processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context: F    Target: i \n",
      " Context: Fi    Target: r \n",
      " Context: Fir    Target: s \n",
      " Context: Firs    Target: t \n",
      " Context: First    Target:   \n",
      " Context: First     Target: C \n",
      " Context: First C    Target: i \n",
      " Context: First Ci    Target: t \n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = decode(x[:i+1].tolist())\n",
    "    target = decode([y[i].tolist()])\n",
    "    print(f''' Context: {context}    Target: {target} ''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  4\n",
    "block_size =  8 #Context Length\n",
    "def get_batch(split):\n",
    "    data = train_data if split=='train' else test_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))  #Gives us random indexes\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #Get consecutive characters of block_size, for each batch\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1078327,  453969,   41646,  671252])\n",
      "64\n",
      "1115394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.randint(len(data)-block_size,(batch_size,)))\n",
    "print(max(data.tolist()))\n",
    "print(len(data))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
      "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
      "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
      "        [43, 57, 58, 63,  6,  1, 58, 46]])\n",
      "Trarget tensor([[43, 60, 43, 52,  1, 63, 43, 39],\n",
      "        [43, 42,  8,  0, 25, 63,  1, 45],\n",
      "        [42,  5, 57,  1, 57, 39, 49, 43],\n",
      "        [57, 58, 63,  6,  1, 58, 46, 47]])\n",
      "Context: s\n",
      "Target: e\n",
      "Context: se\n",
      "Target: v\n",
      "Context: sev\n",
      "Target: e\n",
      "Context: seve\n",
      "Target: n\n",
      "Context: seven\n",
      "Target:  \n",
      "Context: seven \n",
      "Target: y\n",
      "Context: seven y\n",
      "Target: e\n",
      "Context: seven ye\n",
      "Target: a\n",
      "Context: v\n",
      "Target: e\n",
      "Context: ve\n",
      "Target: d\n",
      "Context: ved\n",
      "Target: .\n",
      "Context: ved.\n",
      "Target: \n",
      "\n",
      "Context: ved.\n",
      "\n",
      "Target: M\n",
      "Context: ved.\n",
      "M\n",
      "Target: y\n",
      "Context: ved.\n",
      "My\n",
      "Target:  \n",
      "Context: ved.\n",
      "My \n",
      "Target: g\n",
      "Context: r\n",
      "Target: d\n",
      "Context: rd\n",
      "Target: '\n",
      "Context: rd'\n",
      "Target: s\n",
      "Context: rd's\n",
      "Target:  \n",
      "Context: rd's \n",
      "Target: s\n",
      "Context: rd's s\n",
      "Target: a\n",
      "Context: rd's sa\n",
      "Target: k\n",
      "Context: rd's sak\n",
      "Target: e\n",
      "Context: e\n",
      "Target: s\n",
      "Context: es\n",
      "Target: t\n",
      "Context: est\n",
      "Target: y\n",
      "Context: esty\n",
      "Target: ,\n",
      "Context: esty,\n",
      "Target:  \n",
      "Context: esty, \n",
      "Target: t\n",
      "Context: esty, t\n",
      "Target: h\n",
      "Context: esty, th\n",
      "Target: i\n"
     ]
    }
   ],
   "source": [
    "xb, yb=  get_batch('train')\n",
    "\n",
    "print('Context',xb) #block_size is context\n",
    "print('Trarget',yb)\n",
    "\n",
    "for j in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        # context=enc.decode(list(chain.from_iterable([xb[i][:i+1]].tolist())))\n",
    "        # target= enc.decode([yb[j,i].tolist()]) \n",
    "        print('Context:',decode(xb[j][:i+1].tolist()))\n",
    "        #print('Target:',decode([yb[j][i]])) When using tiktoken\n",
    "        print('Target:',decode([yb[j][i].tolist()]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Bi Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100277, 100277)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tiktoken\n",
    "# token_embedding_table = nn.Embedding(enc.n_vocab,enc.n_vocab)\n",
    "# token_embedding_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we build models when we can leverage tik token for embeddings?\n",
    "    * Embedding are not talking to themselves unless we train them. Currently they can just see themselves       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57, 43, 60, 43, 52,  1, 63, 43],\n",
       "        [60, 43, 42,  8,  0, 25, 63,  1],\n",
       "        [56, 42,  5, 57,  1, 57, 39, 49],\n",
       "        [43, 57, 58, 63,  6,  1, 58, 46]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # Builds an embedding of vocab size with some weights initialized\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C), Batch Time Channels: 4, 8, vocab size # Filtering embedding for specific indices, pluck out a row of token(integer) from embedding space\n",
    "        if targets==None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits=logits.view(B*T, C)\n",
    "            targets=targets.view(B*T) #Stretch out the tensor\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is current B X T \n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:] # Focusing on last character, this makes the model bi-gram model\n",
    "            probs = F.softmax(logits, dim=-1) # Converting logits to prob\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size_manual)\n",
    "logits, loss = m(xb, yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8549, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VMPFsPHSYAHIkPXroxdH\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long),max_new_tokens=20)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.508910655975342\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(40000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Th dst it wife acoupat ayo S ly me o n;\n",
      "CHE glewetlvek ju gh thasold;\n",
      "AUn,\n",
      "LEROLout in my celay ck od Whe aver ze,\n",
      "Th lshoooo CHe d,\n",
      "Fime INRowor m ttharoneroucaulenin lo mprinallay,\n",
      "Thionghout ckererurd t s t ifine, chorn,\n",
      "If,\n",
      "RDUCESSifop ar t berdwice'ell t mp pru monyouroos, di hero ns s myoost fol. and bak hyoupanokicit ay\n",
      "The to talyollous goviton bl IO, ng thigreper fin ope, he hul kispethine's!\n",
      "Thtte mou cther ES:\n",
      "\n",
      "Anoucofod thoo ce bl avishthe wind, y or\n",
      "\n",
      "N so flootinceaned bl atce ck n\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
