{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing the required packages\n",
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "import sentencepiece as spm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer\n",
        "import google\n",
        "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
        "\n",
        "# Command to clone the repo\n",
        "#!git clone \"https://github.com/abhinand5/tamil-llama.git\"\n",
        "\n",
        "# Checking if the correct environment is being picked up\n",
        "sys.executable\n",
        "\n",
        "import os \n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/azureuser/localfiles/envs/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/home/azureuser/localfiles/envs/.env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  _torch_pytree._register_pytree_node(\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/kannada-fm/notebooks'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711086855229
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining path variables\n",
        "base_dir = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/\"\n",
        "data_path = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/kannada-fm/data/\"\n",
        "\n",
        "corpus_output_dir =data_path + \"KannadaCorpus\"\n",
        "corpus_output_file_name = \"kannada_corpus.txt\"\n",
        "corpus_path = os.path.join(corpus_output_dir, corpus_output_file_name)\n",
        "\n",
        "\n",
        "model_output_dir = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/models/KannadaModel/\"\n",
        "model_prefix = \"kannada_sp\"\n",
        "output_model_path = os.path.join(model_output_dir, f\"{model_prefix}.model\")\n",
        "\n",
        "#os.makedirs(corpus_output_dir, exist_ok=True)\n",
        "#os.makedirs(model_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "mistral_tokenizer_dir = \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/\""
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087055442
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentencepiece Model Parameters\n",
        "vocab_size = 10000\n",
        "character_coverage = 1.0\n",
        "model_type = \"unigram\"\n",
        "  "
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711086999175
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "# dataset_name = load_dataset(\"Kannada-LLM-Labs/Wikipedia-Kn\",split=\"train\")\n",
        "# train_df = pd.DataFrame(dataset_name)\n",
        "# train_df.to_csv(data_path+\"Kannada-LLM-Labs/Wikipedia-Kn.csv\", sep=\"|\")\n",
        "text_col = \"text\"\n",
        "\n",
        "# second time\n",
        "train_df = pd.read_csv(data_path+\"Kannada-LLM-Labs/Wikipedia-Kn.csv\", sep=\"|\")"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087071467
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "(31437, 5)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087071550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the text column into a text file\n",
        "with open(corpus_path, \"w\") as file:\n",
        "    for index, value in tqdm(\n",
        "            train_df[text_col].items(), total=len(train_df)):\n",
        "            file.write(str(value) + \"\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r  0%|          | 0/31437 [00:00<?, ?it/s]\r 24%|██▎       | 7423/31437 [00:00<00:00, 74225.65it/s]\r 47%|████▋     | 14846/31437 [00:00<00:00, 37966.03it/s]\r 67%|██████▋   | 20942/31437 [00:00<00:00, 44835.61it/s]\r 90%|█████████ | 28388/31437 [00:00<00:00, 53726.07it/s]\r100%|██████████| 31437/31437 [00:00<00:00, 50726.01it/s]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709544200674
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentencepiece Model Training\n",
        "spm.SentencePieceTrainer.train(\n",
        "        input=corpus_path,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=character_coverage,\n",
        "        model_type=model_type)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: /mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/kannada-fm/data/KannadaCorpus/kannada_corpus.txt\n  input_format: \n  model_prefix: kannada_sp\n  model_type: UNIGRAM\n  vocab_size: 10000\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 0\n  bos_id: 1\n  eos_id: 2\n  pad_id: -1\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: /mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/kannada-fm/data/KannadaCorpus/kannada_corpus.txt\ntrainer_interface.cc(380) LOG(WARNING) Found too long line (4389 > 4192).\ntrainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\ntrainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\ntrainer_interface.cc(409) LOG(INFO) Loaded all 913102 sentences\ntrainer_interface.cc(416) LOG(INFO) Skipped 3978 too long sentences.\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=139563157\ntrainer_interface.cc(550) LOG(INFO) Done: 100% characters are covered.\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=3048\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 901799 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=69715645\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 1003048 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 901799\ntrainer_interface.cc(609) LOG(INFO) Done! 2117484\nunigram_model_trainer.cc(602) LOG(INFO) Using 2117484 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=474378 obj=16.2028 num_tokens=5533272 num_tokens/piece=11.6643\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=388251 obj=13.374 num_tokens=5547434 num_tokens/piece=14.2883\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=290956 obj=13.3364 num_tokens=5614671 num_tokens/piece=19.2973\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=289718 obj=13.3176 num_tokens=5635913 num_tokens/piece=19.4531\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=217281 obj=13.3564 num_tokens=5750812 num_tokens/piece=26.4672\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=217259 obj=13.3409 num_tokens=5751347 num_tokens/piece=26.4723\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=162944 obj=13.4343 num_tokens=5944920 num_tokens/piece=36.4844\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=162941 obj=13.4054 num_tokens=5945209 num_tokens/piece=36.4869\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=122205 obj=13.5665 num_tokens=6168303 num_tokens/piece=50.475\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=122205 obj=13.5226 num_tokens=6168904 num_tokens/piece=50.48\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=91653 obj=13.7347 num_tokens=6404078 num_tokens/piece=69.8731\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=91653 obj=13.6821 num_tokens=6404495 num_tokens/piece=69.8776\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=68739 obj=13.938 num_tokens=6656318 num_tokens/piece=96.8347\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=68739 obj=13.8786 num_tokens=6656956 num_tokens/piece=96.8439\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=51554 obj=14.1779 num_tokens=6925656 num_tokens/piece=134.338\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=51554 obj=14.1119 num_tokens=6926069 num_tokens/piece=134.346\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=38665 obj=14.4625 num_tokens=7213685 num_tokens/piece=186.569\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=38665 obj=14.3872 num_tokens=7214190 num_tokens/piece=186.582\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=28998 obj=14.7975 num_tokens=7536253 num_tokens/piece=259.889\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=28998 obj=14.7087 num_tokens=7536690 num_tokens/piece=259.904\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=21748 obj=15.188 num_tokens=7903511 num_tokens/piece=363.413\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=21748 obj=15.0828 num_tokens=7903834 num_tokens/piece=363.428\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=16311 obj=15.6527 num_tokens=8335356 num_tokens/piece=511.027\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=16311 obj=15.5278 num_tokens=8335734 num_tokens/piece=511.05\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=12233 obj=16.2207 num_tokens=8828039 num_tokens/piece=721.658\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12233 obj=16.0688 num_tokens=8828410 num_tokens/piece=721.688\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=11000 obj=16.3416 num_tokens=9041259 num_tokens/piece=821.933\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=11000 obj=16.2819 num_tokens=9041404 num_tokens/piece=821.946\ntrainer_interface.cc(687) LOG(INFO) Saving model: kannada_sp.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: kannada_sp.vocab\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087213728
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.rename(\n",
        "    f\"{model_prefix}.vocab\",\n",
        "    os.path.join(model_output_dir, f\"{model_prefix}.vocab\"))\n",
        "os.rename(\n",
        "    f\"{model_prefix}.model\",\n",
        "    os.path.join(model_output_dir, f\"{model_prefix}.model\"))"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087235553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the tokenizer for the first time\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "#tokenizer.save_pretrained(mistral_tokenizer_dir)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 10.6MB/s]\ntokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 35.7MB/s]\ntokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 1.87MB/s]\nspecial_tokens_map.json: 100%|██████████| 72.0/72.0 [00:00<00:00, 615kB/s]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 81,
          "data": {
            "text/plain": "('/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-anantha-gpu/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer_config.json',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-anantha-gpu/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/special_tokens_map.json',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-anantha-gpu/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.model',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-anantha-gpu/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/added_tokens.json',\n '/mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-anantha-gpu/code/Users/anantha.rajpurohit/models/mistralai/Mistral-7B-Instruct-v0.2/tokenizer.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1709543107069
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Mistral tokenizer\n",
        "text = \"\"\"<s>[INST] What is your favourite condiment? [/INST]\n",
        "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n",
        "[INST] Do you have mayonnaise recipes? [/INST]\"\"\"\n",
        "# text = \"೧೭ ನೆಯ ಶತಮಾನದಿಂದ ಮುಂದಕ್ಕೆ ಪೋರ್ಚುಗೀಸ್, ಫ್ರೆಂಚ್ ಮತ್ತು ಬ್ರಿಟಿಷ್ ವ್ಯಾಪಾರಿಗಳು ಭಾರತಕ್ಕೆ ಬರಲಾರಂಭಿಸಿದರು. \n",
        "# ಹಂಚಿಹೋಗಿದ್ದ ಭಾರತದ ರಾಜಕೀಯ ಪರಿಸ್ಥಿತಿಯ ಲಾಭ ಪಡೆದು ಭಾರತದ ಅನೇಕ ಪ್ರದೇಶಗಳನ್ನು ಇವರು ವಶಪಡಿಸಿಕೊಳ್ಳಲಾರಂಭಿಸಿದರು.\"\n",
        "print(\"Test text:\\n\", text)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(mistral_tokenizer_dir)\n",
        "mistral_tokenized = tokenizer.tokenize(text)\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file = output_model_path)\n",
        "kannada_sp_tokenized = sp.encode(text, out_type=str)\n",
        "\n",
        "print(f\"Tokenized by Mistral tokenizer:{mistral_tokenized}\")\n",
        "print(f\"Mistral tokenizer n_tokens={len(mistral_tokenized)}\")\n",
        "\n",
        "print(f\"Tokenized by Kannada_sp tokenizer:{kannada_sp_tokenized}\")\n",
        "print(f\"Kannada SP tokenizer n_tokens={len(kannada_sp_tokenized)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test text:\n <s>[INST] What is your favourite condiment? [/INST]\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n[INST] Do you have mayonnaise recipes? [/INST]\nTokenized by Mistral tokenizer:['<s>', '▁[', 'INST', ']', '▁What', '▁is', '▁your', '▁favourite', '▁cond', 'iment', '?', '▁[', '/', 'INST', ']', '<0x0A>', 'Well', ',', '▁I', \"'\", 'm', '▁quite', '▁partial', '▁to', '▁a', '▁good', '▁sque', 'eze', '▁of', '▁fresh', '▁lemon', '▁juice', '.', '▁It', '▁adds', '▁just', '▁the', '▁right', '▁amount', '▁of', '▁z', 'esty', '▁flav', 'our', '▁to', '▁whatever', '▁I', \"'\", 'm', '▁cooking', '▁up', '▁in', '▁the', '▁kitchen', '!', '</s>', '▁', '<0x0A>', '[', 'INST', ']', '▁Do', '▁you', '▁have', '▁may', 'onna', 'ise', '▁recipes', '?', '▁[', '/', 'INST', ']']\nMistral tokenizer n_tokens=73\nTokenized by Kannada_sp tokenizer:['▁', '<', 's', '>', '[', 'I', 'N', 'S', 'T', ']', '▁W', 'h', 'at', '▁', 'is', '▁', 'y', 'our', '▁f', 'av', 'our', 'ite', '▁', 'con', 'di', 'ment', '?', '▁[', '/', 'I', 'N', 'S', 'T', ']', '▁W', 'ell', ',', '▁I', \"'\", 'm', '▁', 'qu', 'ite', '▁p', 'art', 'i', 'al', '▁to', '▁a', '▁', 'g', 'o', 'od', '▁s', 'qu', 'e', 'e', 'z', 'e', '▁of', '▁f', 're', 'sh', '▁l', 'em', 'on', '▁', 'j', 'u', 'ic', 'e', '.', '▁I', 't', '▁', 'ad', 'd', 's', '▁', 'j', 'ust', '▁the', '▁', 'right', '▁', 'am', 'o', 'un', 't', '▁of', '▁', 'z', 'est', 'y', '▁f', 'l', 'av', 'our', '▁to', '▁w', 'h', 'ate', 'ver', '▁I', \"'\", 'm', '▁c', 'ook', 'ing', '▁', 'up', '▁in', '▁the', '▁k', 'it', 'ch', 'en', '!', '</', 's', '>', '▁[', 'I', 'N', 'S', 'T', ']', '▁D', 'o', '▁', 'y', 'o', 'u', '▁h', 'av', 'e', '▁m', 'ay', 'on', 'na', 'is', 'e', '▁re', 'ci', 'pe', 's', '?', '▁[', '/', 'I', 'N', 'S', 'T', ']']\nKannada SP tokenizer n_tokens=154\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087264413
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the Tokenizer\n",
        "\n",
        "# Loading respective tokenizers\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_tokenizer_dir,local_files_only=True,use_fast=False)\n",
        "\n",
        "kannada_sp_model = spm.SentencePieceProcessor()\n",
        "kannada_sp_model.Load(output_model_path)\n",
        "\n",
        "mistral_spm = sp_pb2_model.ModelProto()\n",
        "mistral_spm.ParseFromString(mistral_tokenizer.sp_model.serialized_model_proto())\n",
        "\n",
        "kannada_spm = sp_pb2_model.ModelProto()\n",
        "kannada_spm.ParseFromString(kannada_sp_model.serialized_model_proto())"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "455217"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087308423
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print number of tokens\n",
        "print(len(mistral_tokenizer), len(kannada_sp_model))\n",
        "print(mistral_tokenizer.all_special_tokens)\n",
        "print(mistral_tokenizer.all_special_ids)\n",
        "print(mistral_tokenizer.special_tokens_map)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "32000 10000\n['<s>', '</s>', '<unk>']\n[1, 2, 0]\n{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087618553
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final merging\n",
        "mistral_spm_tokens_set = set(p.piece for p in mistral_spm.pieces)\n",
        "print(len(mistral_spm_tokens_set))\n",
        "print(f\"Before:{len(mistral_spm_tokens_set)}\")\n",
        "\n",
        "for p in kannada_spm.pieces:\n",
        "    piece = p.piece\n",
        "    if piece not in mistral_spm_tokens_set:\n",
        "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
        "        new_p.piece = piece\n",
        "        new_p.score = 0\n",
        "        mistral_spm.pieces.append(new_p)\n",
        "        \n",
        "print(f\"New model pieces: {len(mistral_spm.pieces)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "39863\nBefore:39863\nNew model pieces: 39863\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087623432
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save\n",
        "output_sp_dir = base_dir+\"models/KannadaModel/merged_tokenizer_sp\"\n",
        "output_hf_dir = base_dir+\"models/KannadaModel/merged_tokenizer_hf\"  # the path to save Hindi-Mistral tokenizer"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087642753
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save\n",
        "\n",
        "os.makedirs(output_sp_dir, exist_ok=True)\n",
        "\n",
        "with open(output_sp_dir + \"/kannada_mistral.model\", \"wb\") as f:\n",
        "    # f.write(kannada_spm.SerializeToString())\n",
        "    f.write(mistral_spm.SerializeToString())\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087655516
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LlamaTokenizer(output_sp_dir + \"/kannada_mistral.model\")\n",
        "# AutoTokenizer\n",
        "tokenizer.save_pretrained(output_hf_dir)\n",
        "print(f\"Kannada-Mistral tokenizer has been saved to {output_hf_dir}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Kannada-Mistral tokenizer has been saved to /mnt/batch/tasks/shared/LS_root/mounts/clusters/fm-kannada-main/code/Users/anantha.rajpurohit/models/KannadaModel/merged_tokenizer_hf\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087659395
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_tokenizer_dir)\n",
        "# kannada_mistral_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
        "kannada_mistral_tokenizer = AutoTokenizer.from_pretrained(output_hf_dir,local_files_only=True,use_fast=False)\n",
        "\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.special_tokens_map)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['<s>', '</s>', '<unk>']\n[1, 2, 0]\n{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087728204
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kannada_mistral_tokenized = kannada_mistral_tokenizer.tokenize(text)\n",
        "\n",
        "print(f\"Tokenized by Mistral tokenizer:{kannada_mistral_tokenized}\")\n",
        "print(f\"Mistral tokenizer n_tokens={len(mistral_tokenized)}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Tokenized by Mistral tokenizer:['<s>', '▁[', 'INST', ']', '▁What', '▁is', '▁your', '▁favourite', '▁cond', 'iment', '?', '▁[', '/', 'INST', ']', '<0x0A>', 'Well', ',', '▁I', \"'\", 'm', '▁quite', '▁partial', '▁to', '▁a', '▁good', '▁sque', 'eze', '▁of', '▁fresh', '▁lemon', '▁juice', '.', '▁It', '▁adds', '▁just', '▁the', '▁right', '▁amount', '▁of', '▁z', 'esty', '▁flav', 'our', '▁to', '▁whatever', '▁I', \"'\", 'm', '▁cooking', '▁up', '▁in', '▁the', '▁kitchen', '!', '</s>', '▁', '<0x0A>', '[', 'INST', ']', '▁Do', '▁you', '▁have', '▁may', 'onna', 'ise', '▁recipes', '?', '▁[', '/', 'INST', ']']\nMistral tokenizer n_tokens=73\n"
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087758931
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"೧೭ ನೆಯ ಶತಮಾನದಿಂದ ಮುಂದಕ್ಕೆ ಪೋರ್ಚುಗೀಸ್, ಫ್ರೆಂಚ್ ಮತ್ತು ಬ್ರಿಟಿಷ್ ವ್ಯಾಪಾರಿಗಳು ಭಾರತಕ್ಕೆ ಬರಲಾರಂಭಿಸಿದರು. ಹಂಚಿಹೋಗಿದ್ದ ಭಾರತದ ರಾಜಕೀಯ ಪರಿಸ್ಥಿತಿಯ ಲಾಭ ಪಡೆದು ಭಾರತದ ಅನೇಕ ಪ್ರದೇಶಗಳನ್ನು ಇವರು ವಶಪಡಿಸಿಕೊಳ್ಳಲಾರಂಭಿಸಿದರು.\"\n",
        "\n",
        "print(\"Test text:\\n\", text)\n",
        "mistral_tokenized = mistral_tokenizer.tokenize(text)\n",
        "kannada_mistral_tokenized = kannada_mistral_tokenizer.tokenize(text)\n",
        "\n",
        "print(f\"Tokenized by Mistral tokenizer:{mistral_tokenized}\")\n",
        "print(f\"Mistral tokenizer n_tokens={len(mistral_tokenized)}\")\n",
        "print(f\"Tokenized by Kannada-Mistral tokenizer:{kannada_mistral_tokenized}\")\n",
        "print(f\"Kannada-Mistral tokenizer n_tokens={len(kannada_mistral_tokenized)}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test text:\n ೧೭ ನೆಯ ಶತಮಾನದಿಂದ ಮುಂದಕ್ಕೆ ಪೋರ್ಚುಗೀಸ್, ಫ್ರೆಂಚ್ ಮತ್ತು ಬ್ರಿಟಿಷ್ ವ್ಯಾಪಾರಿಗಳು ಭಾರತಕ್ಕೆ ಬರಲಾರಂಭಿಸಿದರು. ಹಂಚಿಹೋಗಿದ್ದ ಭಾರತದ ರಾಜಕೀಯ ಪರಿಸ್ಥಿತಿಯ ಲಾಭ ಪಡೆದು ಭಾರತದ ಅನೇಕ ಪ್ರದೇಶಗಳನ್ನು ಇವರು ವಶಪಡಿಸಿಕೊಳ್ಳಲಾರಂಭಿಸಿದರು.\nTokenized by Mistral tokenizer:['▁', '<0xE0>', '<0xB3>', '<0xA7>', '<0xE0>', '<0xB3>', '<0xAD>', '▁', 'ನ', 'ೆ', 'ಯ', '▁', '<0xE0>', '<0xB2>', '<0xB6>', 'ತ', 'ಮ', 'ಾ', 'ನ', 'ದ', 'ಿ', 'ಂ', 'ದ', '▁', 'ಮ', 'ು', 'ಂ', 'ದ', 'ಕ', '್', 'ಕ', 'ೆ', '▁', '<0xE0>', '<0xB2>', '<0xAA>', '<0xE0>', '<0xB3>', '<0x8B>', 'ರ', '್', '<0xE0>', '<0xB2>', '<0x9A>', 'ು', 'ಗ', '<0xE0>', '<0xB3>', '<0x80>', 'ಸ', '್', ',', '▁', '<0xE0>', '<0xB2>', '<0xAB>', '್', 'ರ', 'ೆ', 'ಂ', '<0xE0>', '<0xB2>', '<0x9A>', '್', '▁', 'ಮ', 'ತ', '್', 'ತ', 'ು', '▁', '<0xE0>', '<0xB2>', '<0xAC>', '್', 'ರ', 'ಿ', '<0xE0>', '<0xB2>', '<0x9F>', 'ಿ', '<0xE0>', '<0xB2>', '<0xB7>', '್', '▁', 'ವ', '್', 'ಯ', 'ಾ', '<0xE0>', '<0xB2>', '<0xAA>', 'ಾ', 'ರ', 'ಿ', 'ಗ', '<0xE0>', '<0xB2>', '<0xB3>', 'ು', '▁', '<0xE0>', '<0xB2>', '<0xAD>', 'ಾ', 'ರ', 'ತ', 'ಕ', '್', 'ಕ', 'ೆ', '▁', '<0xE0>', '<0xB2>', '<0xAC>', 'ರ', 'ಲ', 'ಾ', 'ರ', 'ಂ', '<0xE0>', '<0xB2>', '<0xAD>', 'ಿ', 'ಸ', 'ಿ', 'ದ', 'ರ', 'ು', '.', '▁', '<0xE0>', '<0xB2>', '<0xB9>', 'ಂ', '<0xE0>', '<0xB2>', '<0x9A>', 'ಿ', '<0xE0>', '<0xB2>', '<0xB9>', '<0xE0>', '<0xB3>', '<0x8B>', 'ಗ', 'ಿ', 'ದ', '್', 'ದ', '▁', '<0xE0>', '<0xB2>', '<0xAD>', 'ಾ', 'ರ', 'ತ', 'ದ', '▁', 'ರ', 'ಾ', '<0xE0>', '<0xB2>', '<0x9C>', 'ಕ', '<0xE0>', '<0xB3>', '<0x80>', 'ಯ', '▁', '<0xE0>', '<0xB2>', '<0xAA>', 'ರ', 'ಿ', 'ಸ', '್', '<0xE0>', '<0xB2>', '<0xA5>', 'ಿ', 'ತ', 'ಿ', 'ಯ', '▁', 'ಲ', 'ಾ', '<0xE0>', '<0xB2>', '<0xAD>', '▁', '<0xE0>', '<0xB2>', '<0xAA>', '<0xE0>', '<0xB2>', '<0xA1>', 'ೆ', 'ದ', 'ು', '▁', '<0xE0>', '<0xB2>', '<0xAD>', 'ಾ', 'ರ', 'ತ', 'ದ', '▁', '<0xE0>', '<0xB2>', '<0x85>', 'ನ', '<0xE0>', '<0xB3>', '<0x87>', 'ಕ', '▁', '<0xE0>', '<0xB2>', '<0xAA>', '್', 'ರ', 'ದ', '<0xE0>', '<0xB3>', '<0x87>', '<0xE0>', '<0xB2>', '<0xB6>', 'ಗ', '<0xE0>', '<0xB2>', '<0xB3>', 'ನ', '್', 'ನ', 'ು', '▁', '<0xE0>', '<0xB2>', '<0x87>', 'ವ', 'ರ', 'ು', '▁', 'ವ', '<0xE0>', '<0xB2>', '<0xB6>', '<0xE0>', '<0xB2>', '<0xAA>', '<0xE0>', '<0xB2>', '<0xA1>', 'ಿ', 'ಸ', 'ಿ', 'ಕ', '<0xE0>', '<0xB3>', '<0x8A>', '<0xE0>', '<0xB2>', '<0xB3>', '್', '<0xE0>', '<0xB2>', '<0xB3>', 'ಲ', 'ಾ', 'ರ', 'ಂ', '<0xE0>', '<0xB2>', '<0xAD>', 'ಿ', 'ಸ', 'ಿ', 'ದ', 'ರ', 'ು', '.']\nMistral tokenizer n_tokens=285\nTokenized by Kannada-Mistral tokenizer:['▁೧೭', '▁ನೆ', 'ಯ', '▁ಶ', 'ತ', 'ಮಾನ', 'ದಿಂದ', '▁ಮುಂದ', 'ಕ್ಕೆ', '▁ಪೋ', 'ರ್', 'ಚು', 'ಗೀ', 'ಸ್', ',', '▁ಫ್ರೆಂಚ್', '▁ಮತ', '್', 'ತು', '▁ಬ', '್', 'ರಿ', 'ಟಿ', 'ಷ್', '▁ವ್ಯಾ', 'ಪಾ', 'ರಿ', 'ಗಳು', '▁ಭಾರತಕ್ಕೆ', '▁ಬರ', 'ಲಾ', 'ರಂ', 'ಭಿ', 'ಸಿ', 'ದ', 'ರು', '.', '▁ಹಂಚ', 'ಿ', 'ಹೋಗ', 'ಿದ', '್', 'ದ', '▁ಭಾರತದ', '▁ರಾಜ', 'ಕೀ', 'ಯ', '▁ಪರಿ', 'ಸ್ಥ', 'ಿತ', 'ಿಯ', '▁ಲಾಭ', '▁ಪಡೆದು', '▁ಭಾರತದ', '▁ಅ', 'ನೇ', 'ಕ', '▁ಪ', '್', 'ರ', 'ದೇಶ', 'ಗಳನ್ನು', '▁ಇವರು', '▁ವಶ', 'ಪಡ', 'ಿಸಿಕೊಳ್ಳ', 'ಲಾ', 'ರಂ', 'ಭಿ', 'ಸಿ', 'ದ', 'ರು', '.']\nKannada-Mistral tokenizer n_tokens=73\n"
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1711087936917
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "py310data",
      "language": "python",
      "display_name": "py310data"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "py310data"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}